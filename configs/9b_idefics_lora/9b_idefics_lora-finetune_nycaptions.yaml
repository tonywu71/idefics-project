experiment_name: 9b_idefics_lora-finetune_nycaptions

# ======== Training args ========
model_dir: checkpoints
dataset_name: newyorker_caption
train_batch_size: 2
eval_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: False
optim: paged_adamw_8bit
learning_rate: 2e-4
warmup_steps:
eval_steps: 40
save_steps: 40
logging_steps: 20
num_train_epochs: 1

# ======== LoRA ========
use_lora: True
lora_rank: 16
lora_alpha: 32
target_modules: ["q_proj", "k_proj", "v_proj"]
lora_dropout: 0.05
bias: "none"

# ======== Other training args ========
# Unchanged -> see `FinetuneConfig` class for default values.
