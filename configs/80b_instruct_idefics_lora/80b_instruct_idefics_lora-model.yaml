# ======== Model config ========
checkpoint: HuggingFaceM4/idefics-80b_instruct
lora_checkpoint:  # add checkpoint dirpath/HuggingFace Hub ID after the model fine-tuning
bf16: True  # change to fp16 instead of bf16 to allow testing on non-Ampere GPUs
load_in_4_bits: True
